import torch
import torch.nn as nn
import torch.nn.functional as F

#This is the architecture used for the vague float encoded setting. It is like the 
#orginial "archs" file with an added linear layer in both Sender and Receiver.


class Sender(nn.Module):
    """
    Sender gets as input targets and distractors in an ordered fashion (targets first).
    It embeds both targets and distractors and returns a joint embedding.
    """

    def __init__(self, n_hidden, n_features, n_targets, context_unaware=False):
        super(Sender, self).__init__()
        # Additional linear layer for replacing hard thresholding
        self.fc_threshold = nn.Linear(n_features * n_targets, n_features * n_targets)
        # layers that were originally implemented
        self.fc1 = nn.Linear(n_features * n_targets, n_hidden)
        self.fc2 = nn.Linear(n_features * n_targets, n_hidden)
        self.fc3 = nn.Linear(2 * n_hidden, n_hidden)
        self.context_unaware = context_unaware

    def forward(self, x, aux_input=None):
        batch_size = x.shape[0]
        n_obj = x.shape[1]
        n_features = x.shape[2]
        n_targets = int(n_obj / 2)

        # Process target features
        targets = x[:, :n_targets]
        targets_flat = targets.reshape(batch_size, n_targets * n_features)
        # Apply the thresholding layer
        targets_flat = self.fc_threshold(targets_flat)
        target_feature_embedding = F.leaky_relu(self.fc1(targets_flat))

        if self.context_unaware:
            return target_feature_embedding

        # Process distractor features similarly if context-aware
        distractors = x[:, n_targets:]
        distractors_flat = distractors.reshape(batch_size, n_targets * n_features)
        # Apply the thresholding layer
        distractors_flat = self.fc_threshold(distractors_flat)
        distractor_feature_embedding = F.leaky_relu(self.fc2(distractors_flat))

        # Combine target and distractor embeddings
        joint_embedding = self.fc3(
            torch.cat([target_feature_embedding, distractor_feature_embedding], dim=1)
        ).tanh()

        return joint_embedding


class Receiver(nn.Module):
    def __init__(self, n_features, n_hidden):
        """
        Receives as input the vector generated by the message-decoding RNN in the wrapper (x)
        and game-specific input (input, i.e. matrix containing all input attribute-value vectors).
        The module maps these vectors to the same dimensionality as the RNN output vector,
        and computes a dot product between the latter and each of the (transformed) input vectors.
        The output dot product list is interpreted as a non-normalized probability distribution
        over possible positions of the target.
        """
        super(Receiver, self).__init__()
        # additional linear layer for thresholding
        self.fc_threshold = nn.Linear(n_features, n_features)
        # embedding layer
        self.fc1 = nn.Linear(n_features, n_hidden)

    def forward(self, x, input, _aux_input=None):
        # from EGG: the rationale for the non-linearity here is that the RNN output will also be the
        # outcome of a non-linearity
        input = self.fc_threshold(input)
        embedded_input = self.fc1(input).tanh()  # [32, 20, 256]

        dots = torch.matmul(embedded_input, torch.unsqueeze(x, dim=-1))
        return dots.squeeze()  # [32, 20]
