{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from utils.load_results import *\n",
    "from utils.analysis_from_interaction import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ('(3,4)', '(3,8)', '(3,16)', '(4,4)', '(4,8)', '(5,4)')\n",
    "n_attributes = (3, 3, 3, 4, 4, 5)\n",
    "n_values = (4, 8, 16, 4, 8, 4)\n",
    "n_epochs = 300\n",
    "paths = [f'results/vague_ds_results/{d}_game_size_10_vsf_3/' for d in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_unaware = False # whether original or context_unaware simulations are evaluated\n",
    "if context_unaware:\n",
    "    setting = 'context_unaware'\n",
    "else:\n",
    "    setting = 'standard'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine Unique Message size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Total message size if symbol order matters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: (3,4)\n",
      "Number of unique messages per run: [119, 90, 73, 78, 152]\n",
      "Mean unique messages: 102.40\n",
      "\n",
      "Dataset: (3,8)\n",
      "Number of unique messages per run: [439, 260, 694, 507, 835]\n",
      "Mean unique messages: 547.00\n",
      "\n",
      "Dataset: (3,16)\n",
      "Number of unique messages per run: [1483, 2147, 1640, 2876, 2765]\n",
      "Mean unique messages: 2182.20\n",
      "\n",
      "Dataset: (4,4)\n",
      "Number of unique messages per run: [718, 1762, 180, 765, 906]\n",
      "Mean unique messages: 866.20\n",
      "\n",
      "Dataset: (4,8)\n",
      "Number of unique messages per run: [16501, 5310, 4778, 8900, 7268]\n",
      "Mean unique messages: 8551.40\n",
      "\n",
      "Dataset: (5,4)\n",
      "Number of unique messages per run: [7437, 5055, 5651, 11539, 6889]\n",
      "Mean unique messages: 7314.20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_runs = 5 # number of runs per ds\n",
    "for i, d in enumerate(datasets):\n",
    "    print(f\"Dataset: {d}\")\n",
    "    unique_messages_all_runs = []\n",
    "\n",
    "    for run in range(num_runs):\n",
    "        path_to_run = paths[i] + '/' + str(setting) + '/' + str(run) + '/'\n",
    "        path_to_interaction_train = (path_to_run + 'interactions/train/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "        interaction = torch.load(path_to_interaction_train)\n",
    "\n",
    "        messages = interaction.message.argmax(dim=-1)\n",
    "        messages = [tuple(msg.tolist()) for msg in messages]  # Convert messages to tuples for hashing\n",
    "        total_messages = set(messages)  # Set removes duplicated concepts. Note: tuples are returned unordered  \n",
    "        number_of_unique_messages = len(total_messages)\n",
    "        unique_messages_all_runs.append(number_of_unique_messages)\n",
    "\n",
    "    # Calculate summary statistics for unique messages\n",
    "    mean_unique_messages = np.mean(unique_messages_all_runs)\n",
    "\n",
    "    print(f'Number of unique messages per run: {unique_messages_all_runs}')\n",
    "    print(f'Mean unique messages: {mean_unique_messages:.2f}')\n",
    "    print()\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Unique message size if you consider messages with the same symbols but in different orders as the same message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: (3,4)\n",
      "Number of unique messages per run: [90, 60, 59, 58, 95]\n",
      "Mean unique sorted messages: 72.40\n",
      "\n",
      "Dataset: (3,8)\n",
      "Number of unique messages per run: [297, 225, 495, 390, 609]\n",
      "Mean unique sorted messages: 403.20\n",
      "\n",
      "Dataset: (3,16)\n",
      "Number of unique messages per run: [1358, 1705, 1416, 2569, 2343]\n",
      "Mean unique sorted messages: 1878.20\n",
      "\n",
      "Dataset: (4,4)\n",
      "Number of unique messages per run: [443, 1098, 145, 498, 490]\n",
      "Mean unique sorted messages: 534.80\n",
      "\n",
      "Dataset: (4,8)\n",
      "Number of unique messages per run: [9395, 4297, 3421, 6000, 4655]\n",
      "Mean unique sorted messages: 5553.60\n",
      "\n",
      "Dataset: (5,4)\n",
      "Number of unique messages per run: [3599, 2793, 3434, 4914, 3066]\n",
      "Mean unique sorted messages: 3561.20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unique_sorted_messages_all_datasets = []\n",
    "\n",
    "for i, d in enumerate(datasets):\n",
    "    print(f\"Dataset: {d}\")\n",
    "    unique_sorted_messages_all_runs = []\n",
    "\n",
    "    for run in range(num_runs):\n",
    "        path_to_run = paths[i] + '/' + str(setting) + '/' + str(run) + '/'\n",
    "        path_to_interaction_train = (path_to_run + 'interactions/train/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "        interaction = torch.load(path_to_interaction_train)\n",
    "\n",
    "        messages = interaction.message.argmax(dim=-1)\n",
    "         # Convert messages to sorted tuples. Sorted orders ints in ascending order.\n",
    "        sorted_messages = [tuple(sorted(msg.tolist())) for msg in messages] \n",
    "        # Set removed duplicates. \n",
    "        unique_sorted_messages = set(sorted_messages)\n",
    "        number_of_unique_sorted_messages = len(unique_sorted_messages) \n",
    "        unique_sorted_messages_all_runs.append(number_of_unique_sorted_messages)\n",
    "    \n",
    "    unique_sorted_messages_all_datasets.append(unique_sorted_messages_all_runs)\n",
    "    mean_unique_sorted_messages = np.mean(unique_sorted_messages_all_runs)\n",
    "    print(f'Number of unique messages per run: {unique_sorted_messages_all_runs}')\n",
    "    print(f'Mean unique sorted messages: {mean_unique_sorted_messages:.2f}')\n",
    "    print() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculates unique message size if you consider messages with the same symbols but in different orders as the same message. Then draws comparison between unique sorted messages with the number of unique concepts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: (3,4)\n",
      "Number of unique messages per run: [90, 60, 59, 58, 95]\n",
      "Number of unique concepts per run: [715, 688, 683, 688, 672]\n",
      "\n",
      "Dataset: (3,8)\n",
      "Number of unique messages per run: [297, 225, 495, 390, 609]\n",
      "Number of unique concepts per run: [2850, 2971, 2848, 2819, 2854]\n",
      "\n",
      "Dataset: (3,16)\n",
      "Number of unique messages per run: [1358, 1705, 1416, 2569, 2343]\n",
      "Number of unique concepts per run: [12750, 12564, 12797, 12971, 12681]\n",
      "\n",
      "Dataset: (4,4)\n",
      "Number of unique messages per run: [443, 1098, 145, 498, 490]\n",
      "Number of unique concepts per run: [5830, 5995, 5935, 6153, 6137]\n",
      "\n",
      "Dataset: (4,8)\n",
      "Number of unique messages per run: [9395, 4297, 3421, 6000, 4655]\n",
      "Number of unique concepts per run: [45246, 45026, 45086, 45252, 44819]\n",
      "\n",
      "Dataset: (5,4)\n",
      "Number of unique messages per run: [3599, 2793, 3434, 4914, 3066]\n",
      "Number of unique concepts per run: [45433, 45306, 45401, 45032, 45226]\n",
      "\n",
      "Dataset: (3,4)\n",
      "Ratios of unique messages to unique concepts per run: [0.1258741258741259, 0.0872093023255814, 0.08638360175695461, 0.08430232558139535, 0.14136904761904762]\n",
      "Mean ratio: 0.11\n",
      "\n",
      "Dataset: (3,8)\n",
      "Ratios of unique messages to unique concepts per run: [0.10421052631578948, 0.07573207674183777, 0.1738061797752809, 0.13834693153600566, 0.21338472319551507]\n",
      "Mean ratio: 0.14\n",
      "\n",
      "Dataset: (3,16)\n",
      "Ratios of unique messages to unique concepts per run: [0.10650980392156863, 0.1357051894301178, 0.11065093381261233, 0.19805720453318942, 0.18476460846936363]\n",
      "Mean ratio: 0.15\n",
      "\n",
      "Dataset: (4,4)\n",
      "Ratios of unique messages to unique concepts per run: [0.07598627787307033, 0.18315262718932443, 0.02443133951137321, 0.08093612871769869, 0.07984357177774157]\n",
      "Mean ratio: 0.09\n",
      "\n",
      "Dataset: (4,8)\n",
      "Ratios of unique messages to unique concepts per run: [0.20764266454493216, 0.09543374938924178, 0.07587721243845096, 0.13259082471492972, 0.10386220129855642]\n",
      "Mean ratio: 0.12\n",
      "\n",
      "Dataset: (5,4)\n",
      "Ratios of unique messages to unique concepts per run: [0.0792155481698325, 0.061647463912064625, 0.07563710050439418, 0.10912240184757506, 0.06779286251271392]\n",
      "Mean ratio: 0.08\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unique_sorted_messages_all_datasets = []\n",
    "unique_concepts_all_datasets = []\n",
    "\n",
    "for i, d in enumerate(datasets):\n",
    "    print(f\"Dataset: {d}\")\n",
    "    unique_sorted_messages_all_runs = []\n",
    "    unique_concepts_all_runs = []\n",
    "    \n",
    "    for run in range(num_runs):\n",
    "        path_to_run = paths[i] + '/' + str(setting) + '/' + str(run) + '/'\n",
    "        path_to_interaction_train = (path_to_run + 'interactions/train/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "        interaction = torch.load(path_to_interaction_train)\n",
    "        \n",
    "        # Calculate unique sorted messages\n",
    "        messages = interaction.message.argmax(dim=-1)\n",
    "        sorted_messages = [tuple(sorted(msg.tolist())) for msg in messages]\n",
    "        unique_sorted_messages = set(sorted_messages)\n",
    "        number_of_unique_sorted_messages = len(unique_sorted_messages)\n",
    "        unique_sorted_messages_all_runs.append(number_of_unique_sorted_messages)\n",
    "        \n",
    "        # Calculate unique concepts\n",
    "        sender_input = interaction.sender_input\n",
    "        n_targets = int(sender_input.shape[1]/2)\n",
    "        target_objects = sender_input[:, :n_targets]\n",
    "        target_objects = k_hot_to_attributes(target_objects, n_values[i])\n",
    "        (objects, fixed) = retrieve_concepts_sampling(target_objects, all_targets=True)\n",
    "        concepts = list(zip(objects, fixed))\n",
    "        concepts_strings = [(str(obj), str(fixed_vec)) for obj, fixed_vec in concepts]\n",
    "        unique_concepts = set(concepts_strings)\n",
    "        number_of_unique_concepts = len(unique_concepts)\n",
    "        unique_concepts_all_runs.append(number_of_unique_concepts)\n",
    "        \n",
    "    unique_sorted_messages_all_datasets.append(unique_sorted_messages_all_runs)\n",
    "    unique_concepts_all_datasets.append(unique_concepts_all_runs)\n",
    "    \n",
    "    print(f'Number of unique messages per run: {unique_sorted_messages_all_runs}')\n",
    "    print(f'Number of unique concepts per run: {unique_concepts_all_runs}')\n",
    "    print()\n",
    "\n",
    "# Calculate the ratio for each run in each dataset\n",
    "for i, d in enumerate(datasets):\n",
    "    print(f\"Dataset: {d}\")\n",
    "    ratios = []\n",
    "    \n",
    "    for run in range(num_runs):\n",
    "        num_unique_messages = unique_sorted_messages_all_datasets[i][run]\n",
    "        num_unique_concepts = unique_concepts_all_datasets[i][run]\n",
    "        ratio = num_unique_messages / num_unique_concepts\n",
    "        ratios.append(ratio)\n",
    "        \n",
    "    print(f'Ratios of unique messages to unique concepts per run: {ratios}')\n",
    "    mean_ratio = np.mean(ratios)\n",
    "    print(f'Mean ratio: {mean_ratio:.2f}')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emergab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
