{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T14:07:31.005473Z",
     "start_time": "2024-03-14T14:07:31.002334Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from utils.analysis_from_interaction import *\n",
    "from egg.core.language_analysis import Disent\n",
    "from language_analysis_local import TopographicSimilarityConceptLevel, encode_target_concepts_for_topsim\n",
    "from utils.analysis_tools_lazimpa import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculate metrics from stored interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T14:07:35.486572Z",
     "start_time": "2024-03-14T14:07:35.482298Z"
    }
   },
   "outputs": [],
   "source": [
    "datasets = ('(3,4)', '(3,8)', '(3,16)', '(4,4)', '(4,8)', '(5,4)')\n",
    "n_attributes = (3, 3, 3, 4, 4, 5)\n",
    "n_values = (4, 8, 16, 4, 8, 4)\n",
    "n_epochs = 300\n",
    "#paths = ['results/' + d + '_sample_scaling_10_balanced_False_vsf_3/' for d in datasets]\n",
    "paths = ['results/' + d + '_game_size_10_vsf_3/' for d in datasets]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-06T14:06:31.077419Z",
     "start_time": "2024-03-06T14:06:31.064730Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datasets = ('(4,4)',)\n",
    "n_attributes = (4,)\n",
    "n_values = (4,)\n",
    "n_epochs = 100\n",
    "#paths = ['results/' + d + '_sample_scaling_10_balanced_False_vsf_3/' for d in datasets]\n",
    "paths = ['results/' + d + '_game_size_10_vsf_3/' for d in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['(3,4)',]\n",
    "n_values = [4,]\n",
    "n_attributes = [3,]\n",
    "n_epochs = 300\n",
    "paths = ['results/' + d + '_game_size_10_vsf_3' for d in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lazy_context_aware'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_unaware = False # whether original or context_unaware simulations are evaluated\n",
    "lazy = True # whether the lazy agent is used\n",
    "impatient = False\n",
    "setting = \"\"\n",
    "if not lazy:\n",
    "    start = 'impatience_' if impatient else ''\n",
    "    if context_unaware:\n",
    "        setting = start + 'context_unaware'\n",
    "    else:\n",
    "        setting = start + 'context_aware'\n",
    "        if impatient == False:\n",
    "            setting = 'standard'\n",
    "else:\n",
    "    start = 'lazimpa' if impatient else 'lazy'\n",
    "    if context_unaware:\n",
    "        setting = start + '_context_unaware'\n",
    "    else:\n",
    "        setting = start + '_context_aware'\n",
    "setting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## entropy scores: MI, effectiveness, efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-06T14:06:37.646673Z",
     "start_time": "2024-03-06T14:06:33.540807Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/(3,4)_game_size_10_vsf_3/lazy_context_aware/0/interactions/train/epoch_300/interaction_gpu0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\OneDrive\\Dokumente\\UNI Osnabrück\\bachelorthesis\\InfosKristina\\Code\\emergent-abstractions\\utils\\analysis_from_interaction.py:315: RuntimeWarning: invalid value encountered in divide\n",
      "  (m_entropy_concept_x_context + c_entropy_concept_x_context - joint_entropy_concept_x_context)\n",
      "d:\\OneDrive\\Dokumente\\UNI Osnabrück\\bachelorthesis\\InfosKristina\\Code\\emergent-abstractions\\utils\\analysis_from_interaction.py:324: RuntimeWarning: invalid value encountered in divide\n",
      "  normalized_effectiveness_conc_x_cont = ((joint_entropy_concept_x_context - m_entropy_concept_x_context)\n",
      "d:\\OneDrive\\Dokumente\\UNI Osnabrück\\bachelorthesis\\InfosKristina\\Code\\emergent-abstractions\\utils\\analysis_from_interaction.py:331: RuntimeWarning: invalid value encountered in divide\n",
      "  normalized_consistency_conc_x_cont = (\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'normalized_mutual_info': 0.849702099903696,\n",
       " 'normalized_mutual_info_hierarchical': array([0.9698283 , 0.90870313, 0.85054945]),\n",
       " 'normalized_mutual_info_context_dep': array([0.84266426, 0.91582143, 0.96725142]),\n",
       " 'normalized_mutual_info_concept_x_context': array([0.9698283 ,        nan,        nan, 0.90578833, 0.9790615 ,\n",
       "               nan, 0.86714186, 0.90961943, 0.96725142]),\n",
       " 'effectiveness': 0.8617319789058988,\n",
       " 'effectiveness_hierarchical': array([1.        , 0.94716929, 0.91317087]),\n",
       " 'effectiveness_context_dep': array([0.78426704, 0.9089747 , 0.97858665]),\n",
       " 'effectiveness_concept_x_context': array([1.        ,        nan,        nan, 0.90240457, 0.99334927,\n",
       "               nan, 0.8441512 , 0.94125095, 0.97858665]),\n",
       " 'consistency': 0.8380034736049569,\n",
       " 'consistency_hierarchical': array([0.94142394, 0.87323939, 0.79596548]),\n",
       " 'consistency_context_dep': array([0.91045775, 0.92277208, 0.95617579]),\n",
       " 'consistency_concept_x_context': array([0.94142394,        nan,        nan, 0.90919757, 0.96517891,\n",
       "               nan, 0.8914199 , 0.8800448 , 0.95617579])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for d in range(len(datasets)):\n",
    "    \n",
    "    for run in range(1): #5\n",
    "\n",
    "        path_to_run = paths[d] + str(setting) +'/' + str(run) + '/' \n",
    "        interaction = load_interaction(paths[d],setting,run,n_epochs)\n",
    "\n",
    "        attributes = n_attributes[d]\n",
    "        values = n_values[d]\n",
    "        scores = information_scores(interaction, attributes, values, normalizer=\"arithmetic\")\n",
    "\n",
    "        #pickle.dump(scores, open(path_to_run + 'entropy_scores.pkl', 'wb'))\n",
    "scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  message length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T16:14:53.162856Z",
     "start_time": "2024-02-28T16:14:51.541390Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/(3,4)_game_size_10_vsf_3/lazy_context_aware/0/interactions/train/epoch_300/interaction_gpu0\n",
      "message length tensor([1, 1, 1,  ..., 1, 1, 2])\n",
      "hierarchical [1.607, 1.544, 1.53]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.607, 1.544, 1.53]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we evaluated message length per hierarchy level after training but \n",
    "# you can also use the HierarchicalMessageLength callback and store the results \n",
    "# TODO: Message length results look weird, needs to be fixed!\n",
    "\n",
    "for d in range(len(datasets)):\n",
    "    \n",
    "    for run in range(1): #5\n",
    "        \n",
    "        path_to_run = paths[d] + str(setting) +'/' + str(run) + '/' \n",
    "        interaction = load_interaction(paths[d],setting,run,n_epochs)\n",
    "\n",
    "        attributes = n_attributes[d]\n",
    "        values = n_values[d]\n",
    "        scores = message_length_per_hierarchy_level(interaction, attributes)\n",
    "        \n",
    "        #pickle.dump(scores, open(path_to_run + 'message_length_hierarchical.pkl', 'wb'))\n",
    "scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  symbol redundancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T16:14:57.892753Z",
     "start_time": "2024-02-28T16:14:53.164028Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m attributes \u001b[38;5;241m=\u001b[39m n_attributes[d]\n\u001b[0;32m      4\u001b[0m values \u001b[38;5;241m=\u001b[39m n_values[d]\n\u001b[1;32m----> 5\u001b[0m vs_factor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m vocab_size \u001b[38;5;241m=\u001b[39m (n_values[d] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m vs_factor \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m run \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m): \u001b[38;5;66;03m#5\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: '_'"
     ]
    }
   ],
   "source": [
    "for d in range(len(datasets)):\n",
    "    \n",
    "    attributes = n_attributes[d]\n",
    "    values = n_values[d]\n",
    "    vs_factor = int(paths[d][-2])\n",
    "    vocab_size = (n_values[d] + 1) * vs_factor + 1\n",
    "    \n",
    "    for run in range(1): #5\n",
    "        \n",
    "        path_to_run = paths[d] + str(setting) +'/' + str(run) + '/'\n",
    "        #symbol_f = np.load(path_to_run + 'symbols_pernsame.npy')\n",
    "        interaction = load_interaction(paths[d],setting,run,n_epochs)\n",
    "        redundancy, MI = symbol_frequency(interaction, attributes, values, vocab_size)\n",
    "        \n",
    "        scores = {'symbol_redundancy': redundancy, 'MI_symbol-attribute_value': MI}\n",
    "        \n",
    "        #pickle.dump(scores, open(path_to_run + 'symbol_redundancy.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ZLA significance scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/(3,4)_game_size_10_vsf_3/lazy_context_aware/0/interactions/train/epoch_300/interaction_gpu0\n",
      "{'mean_message_length': tensor(1.6708), 'mean_weighted_message_length': tensor(1.5368), 'p_zla': tensor(0.0040)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_message_length': tensor(2.0189),\n",
       " 'mean_weighted_message_length': tensor(1.5368),\n",
       " 'p_zla': tensor(0.)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for d in range(len(datasets)):\n",
    "    \n",
    "    for run in range(1): # later 5\n",
    "\n",
    "        path_to_run = paths[d] + str(setting) +'/' + str(run) + '/' \n",
    "        interaction = load_interaction(paths[d],setting,run,n_epochs)\n",
    "\n",
    "        scores = ZLA_significance_score(interaction,num_permutations=1000,remove_after_eos=True)\n",
    "        scores_with_after_eos = ZLA_significance_score(interaction,num_permutations=1000,remove_after_eos=False)\n",
    "\n",
    "        #pickle.dump(scores, open(path_to_run + 'ZLA_significance.pkl', 'wb'))\n",
    "print(scores_with_after_eos)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symbol Informativeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['(3,4)',]\n",
    "n_values = [4,]\n",
    "n_attributes = [3,]\n",
    "n_epochs = 300\n",
    "paths = ['results/' + d + '_game_size_10_vsf_3' for d in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_unaware = False # whether original or context_unaware simulations are evaluated\n",
    "lazy = True # whether the lazy agent is used\n",
    "impatient = False\n",
    "setting = \"\"\n",
    "if not lazy:\n",
    "    start = 'impatience_' if impatient else ''\n",
    "    if context_unaware:\n",
    "        setting = start + 'context_unaware'\n",
    "    else:\n",
    "        setting = start + 'context_aware'\n",
    "        if impatient == False:\n",
    "            setting = 'standard'\n",
    "else:\n",
    "    start = 'lazimpa' if impatient else 'lazy'\n",
    "    if context_unaware:\n",
    "        setting = start + '_context_unaware'\n",
    "    else:\n",
    "        setting = start + '_context_aware'\n",
    "setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/(3,4)_game_size_10_vsf_3/lazy_context_aware/0/interactions/train/epoch_300/interaction_gpu0\n",
      "\n",
      " Max (normal) switch:  {'positional_encoding': tensor([0.9906, 0.9408, 0.9138]), 'effective_length': tensor(1.4947), 'information_density': tensor(0.9726)}\n",
      "\n",
      "Unimportant switch:  {'positional_encoding': tensor([0.0022, 0.0000, 0.0000]), 'effective_length': tensor(0.0022), 'information_density': tensor(0.0014)}\n",
      "Number of important symbols:  tensor(4)\n"
     ]
    }
   ],
   "source": [
    "# Test if change of two unimportant values, without changing the actual symbol has an effect\n",
    "interaction = load_interaction(paths[0],setting,0,n_epochs)\n",
    "attributes = n_attributes[0]\n",
    "values = n_values[0]\n",
    "listener,_,_ = load_listener(paths[0],setting,0,attributes, values,context_unaware)\n",
    "eos_token = 0.0\n",
    "\n",
    "# symbol informativeness\n",
    "messages = interaction.message\n",
    "num_messages, message_length, vocab_size = messages.shape\n",
    "\n",
    "# values to be switched\n",
    "max_values, original_messages = messages.max(dim=-1)\n",
    "\n",
    "eos_mask_total = eos_token != original_messages\n",
    "\n",
    "# only values before eos, to switch\n",
    "eos_excluded_cumulative = torch.cat([ eos_mask_total[:,:i+1].sum(dim=1).unsqueeze(1) == i+1 for i in range(message_length) ],dim=1)\n",
    "\n",
    "# until eos, evaluating changes\n",
    "eos_included_cumualtive = torch.cat([torch.ones_like(eos_excluded_cumulative[:,-1]).unsqueeze(1).bool(), eos_excluded_cumulative[:,:-1]],dim=1)\n",
    "\n",
    "# Prediction for original messages, exclude prediction for values after eos\n",
    "prediction = (listener(messages,interaction.receiver_input) > 0).float()\n",
    "empty_prediction = torch.zeros_like(prediction)\n",
    "prediction = torch.where(eos_included_cumualtive.unsqueeze(-1),prediction,empty_prediction)\n",
    "\n",
    "Lambda_m_k_list = []\n",
    "\n",
    "for i in range(message_length-1): # index = -1 is always eos and not changed. \n",
    "\n",
    "    # pick random new symbol index to switch with\n",
    "    switch_index = torch.randint(1,vocab_size,(num_messages,))\n",
    "\n",
    "    # make sure switch_index != original_messages\n",
    "    same_indices = (switch_index == original_messages[:,i])\n",
    "    while same_indices.any():\n",
    "        switch_index[same_indices] = torch.randint(1, vocab_size, (same_indices.sum().item(),))\n",
    "        same_indices = (switch_index == original_messages[:,i])\n",
    "\n",
    "    # values to switch with\n",
    "    new_value = torch.gather(messages[:,i,:],-1,switch_index.unsqueeze(-1)).squeeze()\n",
    "\n",
    "    # pick random new symbol index to switch with #2 for switching in background\n",
    "    switch_index2 = torch.randint(1,vocab_size,(num_messages,))\n",
    "\n",
    "    # make sure switch_index != original_messages and != switch_index\n",
    "    same_indices = (switch_index2 == original_messages[:,i]) + (switch_index2 == switch_index)\n",
    "    while same_indices.any():\n",
    "        switch_index2[same_indices] = torch.randint(1, vocab_size, (same_indices.sum().item(),))\n",
    "        same_indices = (switch_index2 == original_messages[:,i]) + (switch_index2 == switch_index)\n",
    "\n",
    "    # values to switch with\n",
    "    new_value2 = torch.gather(messages[:,i,:],-1,switch_index2.unsqueeze(-1)).squeeze()\n",
    "\n",
    "    messages_manipulated = messages.clone()\n",
    "\n",
    "    # if eosed, take old value in so no switch (with 2 not max)\n",
    "    new_value_without_eos = torch.where(eos_excluded_cumulative[:,i].squeeze(), new_value,new_value2)\n",
    "    new_value2_without_eos = torch.where(eos_excluded_cumulative[:,i].squeeze(), new_value2,new_value)\n",
    "\n",
    "    # switch value i of all messages to new symbol # not max\n",
    "    messages_manipulated[:,i].scatter_(-1,switch_index2.unsqueeze(-1),new_value_without_eos.unsqueeze(-1))\n",
    "    messages_manipulated[:,i].scatter_(-1,switch_index.unsqueeze(-1),new_value2_without_eos.unsqueeze(-1))\n",
    "\n",
    "    prediction_manipulated = (listener(messages_manipulated,interaction.receiver_input) > 0).float() # bigger 0 means thinks its a concept object\n",
    "    prediction_manipulated = torch.where(eos_included_cumualtive.unsqueeze(-1),prediction_manipulated,empty_prediction)\n",
    "\n",
    "    # compare whether same classification not same values\n",
    "    # only include predictions for symbols that are not eosed yet.\n",
    "    Lambda_m_k_position = (prediction != prediction_manipulated).sum(dim=(1,2)).unsqueeze(1).bool() # (num_messages, 1 )\n",
    "    Lambda_m_k_list.append(Lambda_m_k_position)\n",
    "\n",
    "Lambda_m_k = torch.cat(Lambda_m_k_list,dim=1)\n",
    "eos_mask = eos_excluded_cumulative[:,:-1] # both should be the same shape\n",
    "\n",
    "scores = information_analysis(listener,interaction,eos_token=0.0)\n",
    "print(\"\\n Max (normal) switch: \", scores)\n",
    "\n",
    "return_dict = {\n",
    "    \"positional_encoding\" : positional_encoding(Lambda_m_k, eos_mask),\n",
    "    \"effective_length\" : effective_length(Lambda_m_k, eos_mask),\n",
    "    \"information_density\" : information_density(Lambda_m_k,eos_mask)\n",
    "}\n",
    "print(\"\\nUnimportant switch: \", return_dict)\n",
    "print(\"Number of important symbols: \", Lambda_m_k.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/(3,4)_game_size_10_vsf_3/lazy_context_aware/0/interactions/train/epoch_300/interaction_gpu0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'positional_encoding': tensor([0.9906, 0.9309, 0.9828]),\n",
       " 'effective_length': tensor(1.4920),\n",
       " 'information_density': tensor(0.9708)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normal symbol information analysis\n",
    "for d in range(len(datasets)):\n",
    "    \n",
    "    for run in range(1): # later 5\n",
    "\n",
    "        path_to_run = paths[d] + str(setting) +'/' + str(run) + '/' \n",
    "        interaction = load_interaction(paths[d],setting,run,n_epochs)\n",
    "\n",
    "        attributes = n_attributes[d]\n",
    "        values = n_values[d]\n",
    "\n",
    "        listener,_,_ = load_listener(paths[d],setting,run,attributes, values,context_unaware)\n",
    "\n",
    "        scores = information_analysis(listener,interaction,eos_token=0.0)\n",
    "\n",
    "        #pickle.dump(scores, open(path_to_run + 'symbol_informativeness.pkl', 'wb'))\n",
    "\n",
    "scores # reprintet ohne remove after eos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/(3,4)_game_size_10_vsf_3/lazy_context_aware/0/interactions/train/epoch_300/interaction_gpu0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'positional_encoding': tensor([0.9934, 0.9529, 0.8966]),\n",
       " 'effective_length': tensor(1.5030),\n",
       " 'information_density': tensor(0.9780)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d,run = 0,0\n",
    "interaction = load_interaction(paths[d],setting,run,n_epochs)\n",
    "\n",
    "attributes = n_attributes[d]\n",
    "values = n_values[d]\n",
    "\n",
    "listener,_,_ = load_listener(paths[d],setting,run,attributes, values,context_unaware)\n",
    "\n",
    "information_analysis(listener,interaction,eos_token=0.0) # geprintet mit remove_after_eos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/(3,4)_game_size_10_vsf_3/lazy_context_aware/0/interactions/train/epoch_300/interaction_gpu0\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.])\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.])\n",
      "tensor([0.0000e+00, 0.0000e+00, 4.7684e-07,  ..., 0.0000e+00, 3.3722e-03,\n",
      "        1.0000e+00])\n",
      "tensor([0.0000e+00, 0.0000e+00, 4.7684e-07,  ..., 0.0000e+00, 3.3722e-03,\n",
      "        0.0000e+00])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([2.0000, 2.0000, 2.0000,  ..., 2.0000, 2.0067, 3.0000])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# todo debugging only\n",
    "interaction = load_interaction(paths[d],setting,run,n_epochs)\n",
    "listener,_,_ = load_listener(paths[d],setting,run,attributes, values,context_unaware)\n",
    "message = interaction.message\n",
    "receiver_output = listener(message,interaction.receiver_input)\n",
    "\n",
    "impatience = True\n",
    "\n",
    "not_eosed_before = torch.ones(receiver_output.size(0))\n",
    "expected_length = 0.0 # final includes eos here\n",
    "z = 0.0\n",
    "\n",
    "for step in range(receiver_output.size(1)):\n",
    "    eos_mask = message[:, step, 0]  # always eos == 0\n",
    "    if impatience:\n",
    "        add_mask = not_eosed_before\n",
    "    else:\n",
    "        add_mask = eos_mask * not_eosed_before\n",
    "    z += add_mask\n",
    "    expected_length += add_mask.detach() * (1.0 + step)\n",
    "    print(not_eosed_before)\n",
    "    not_eosed_before = not_eosed_before * (1.0 - eos_mask)\n",
    "\n",
    "expected_length += (step + 1) * not_eosed_before\n",
    "z += not_eosed_before\n",
    "assert z.allclose(\n",
    "            z\n",
    "        ), f\"lost probability mass, {z.min()}, {z.max()}\"\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis for attribute visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  compositionality scores: topsim, posdis, bosdis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### topsim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-06T18:20:39.525914Z",
     "start_time": "2024-03-06T14:09:08.937473Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# topsim\n",
    "# although topsim values are stored throughout training if callbacks are verbose, we reevaluate the\n",
    "# final topsim scores with more data points \n",
    "\n",
    "samples = 5000 # maybe shuffle from these because otherwise I just take the first 5,000 (which might not be the best)\n",
    "for d, dataset in enumerate(datasets):\n",
    "    \n",
    "    dim = [n_values[d]]*n_attributes[d]\n",
    "    \n",
    "    for run in range(5):\n",
    "        print(\"dataset\", dataset, \"run\", run)\n",
    "        \n",
    "        topsim_final = {}\n",
    "        path_to_run = paths[d] + str(setting) +'/' + str(run) + '/'\n",
    "        path_to_interaction_train = (path_to_run + 'interactions/train/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "        path_to_interaction_val = (path_to_run + 'interactions/validation/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "        \n",
    "        TOPSIM = TopographicSimilarityConceptLevel(dim, is_gumbel=True)\n",
    "        \n",
    "        for mode in ['train', 'val']:\n",
    "\n",
    "            if mode == 'train':\n",
    "                interaction = torch.load(path_to_interaction_train)\n",
    "            elif mode == 'val':\n",
    "                interaction = torch.load(path_to_interaction_val)\n",
    "                \n",
    "                  \n",
    "            messages = interaction.message.argmax(dim=-1)\n",
    "            sender_input = interaction.sender_input\n",
    "            n_targets = int(sender_input.shape[1]/2)\n",
    "            # get target objects and fixed vectors to re-construct concepts\n",
    "            target_objects = sender_input[:, :n_targets]\n",
    "            target_objects = k_hot_to_attributes(target_objects, n_values[d])\n",
    "            # concepts are defined by a list of target objects (here one sampled target object) and a fixed vector\n",
    "            (objects, fixed) = retrieve_concepts_sampling(target_objects)\n",
    "            # add one such that zero becomes an empty attribute for the calculation (_)\n",
    "            objects = objects + 1\n",
    "            concepts = torch.from_numpy(objects * (np.array(fixed)))\n",
    "            specific_idx = np.where(np.sum(fixed, axis=1)==n_attributes[d])[0]\n",
    "            messages_specific = messages[specific_idx]\n",
    "            concepts_specific = concepts[specific_idx]\n",
    "            \n",
    "            generic_idx = np.where(np.sum(fixed, axis=1)==1)[0]\n",
    "            messages_generic = messages[generic_idx]\n",
    "            concepts_generic = concepts[generic_idx]\n",
    "\n",
    "            messages = [msg.tolist() for msg in messages]\n",
    "            messages_specific = [msg.tolist() for msg in messages_specific]\n",
    "            messages_generic = [msg.tolist() for msg in messages_generic]\n",
    "\n",
    "            encoded_input = encode_target_concepts_for_topsim(sender_input)\n",
    "            # randomly take samples when more than 5000 samples are available\n",
    "            # if len(encoded_input) > samples: \n",
    "            #     print(\"sampling\")\n",
    "            #     sample_indices = random.sample(range(len(encoded_input)), samples)\n",
    "            #     sampled_input = [encoded_input[i] for i in sample_indices]\n",
    "            #     sampled_messages = [messages[i] for i in sample_indices]\n",
    "            #     print(\"start computing\")\n",
    "            #     print(len(sampled_input), len(sampled_input[0]), len(sampled_input[0][0]))\n",
    "            #     topsim = TOPSIM.compute_topsim(sampled_input, sampled_messages)\n",
    "            # else:\n",
    "            topsim = TOPSIM.compute_topsim(encoded_input[0:samples], messages[0:samples]) # default: hausdorff distance for concepts, edit distance for messages\n",
    "            # if len(concepts_specific) > samples:\n",
    "            #     print(\"sampling specific\")\n",
    "            #     sample_indices_specific = random.sample(range(len(concepts_specific)), samples)\n",
    "            #     sampled_input_specific = [concepts_specific[i] for i in sample_indices_specific]\n",
    "            #     sampled_messages_specific = [messages_specific[i] for i in sample_indices_specific]\n",
    "            #     topsim_specific = TOPSIM.compute_topsim(sampled_input_specific, sampled_messages_specific, \n",
    "            #                                             meaning_distance_fn=\"edit\")\n",
    "            # else:\n",
    "            topsim_specific = TOPSIM.compute_topsim(concepts_specific[0:samples], messages_specific[0:samples], \n",
    "                                                        meaning_distance_fn=\"edit\")\n",
    "            \n",
    "            topsim_generic = TOPSIM.compute_topsim(concepts_generic[0:samples], messages_generic[0:samples],\n",
    "                                                   meaning_distance_fn=\"edit\")\n",
    "\n",
    "            print('... topsim computed')\n",
    "\n",
    "            topsim_final['topsim_' + mode] = topsim\n",
    "            topsim_final['topsim_specific_' + mode] = topsim_specific\n",
    "            topsim_final['topsim_generic_' + mode] = topsim_generic\n",
    "    \n",
    "        pickle.dump(topsim_final, open(path_to_run +  \"topsim_final.pkl\", \"wb\" ) )\n",
    "        print(topsim_final)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Topsim over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-14T14:07:47.776462Z"
    },
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "for d, dataset in enumerate(datasets):\n",
    "    \n",
    "    for run in range(5):\n",
    "        print(\"dataset\", dataset, \"run\", run)\n",
    "        \n",
    "        path_to_run = paths[d] + str(setting) +'/' + str(run) + '/'\n",
    "        path_to_interaction_train = (path_to_run + 'interactions/train/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "        path_to_interaction_val = (path_to_run + 'interactions/validation/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "        \n",
    "        for mode in ['train', 'val']:\n",
    "\n",
    "            if mode == 'train':\n",
    "                interaction = torch.load(path_to_interaction_train)\n",
    "            elif mode == 'val':\n",
    "                interaction = torch.load(path_to_interaction_val)\n",
    "\n",
    "        messages = interaction.message.argmax(dim=-1)\n",
    "        sender_input = interaction.sender_input\n",
    "        messages = [msg.tolist() for msg in messages]\n",
    "        encoded_input = encode_target_concepts_for_topsim(sender_input)\n",
    "        dim = [n_values[0]] * n_attributes[0]\n",
    "        TOPSIM = TopographicSimilarityConceptLevel(dim, is_gumbel=True)\n",
    "        \n",
    "        samples = 5000\n",
    "        num_batches = len(messages) // samples + (len(messages) % samples > 0)\n",
    "        topsim_over_time = []\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "            messages_batch = messages[i * samples:(i + 1) * samples]\n",
    "            topsim = TOPSIM.compute_topsim(encoded_input[i * samples:(i + 1) * samples], messages_batch)\n",
    "            topsim_over_time.append(topsim)\n",
    "            \n",
    "        pickle.dump(topsim_over_time, open(path_to_run +  \"topsim_over_time.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Posdis and Bosdis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T16:16:18.597875Z",
     "start_time": "2024-02-28T16:14:57.896059Z"
    }
   },
   "outputs": [],
   "source": [
    "# use Disent callback from egg\n",
    "\n",
    "for d in range(len(datasets)): \n",
    "    \n",
    "    path = paths[d]\n",
    "    dim = [n_values[d]] * n_attributes[d]\n",
    "    n_features = n_attributes[d] * n_values[d]\n",
    "    vs_factor = int(path[-2])\n",
    "    vocab_size = (n_values[d] + 1) * vs_factor + 1\n",
    "    \n",
    "    print(\"data set\", dim)\n",
    "    \n",
    "    for run in range(5):\n",
    "        \n",
    "        posdis_bosdis = {}\n",
    "    \n",
    "        path_to_run = paths[d] + '/' + str(setting) +'/' + str(run) + '/'\n",
    "        path_to_interaction_train = (path_to_run + 'interactions/train/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "        interaction = torch.load(path_to_interaction_train)\n",
    "        \n",
    "        messages = interaction.message.argmax(dim=-1)\n",
    "        sender_input = interaction.sender_input\n",
    "        n_targets = int(sender_input.shape[1]/2)\n",
    "        # get target objects and fixed vectors to re-construct concepts\n",
    "        target_objects = sender_input[:, :n_targets]\n",
    "        target_objects = k_hot_to_attributes(target_objects, n_values[d])\n",
    "        # concepts are defined by a list of target objects (here one sampled target object) and a fixed vector\n",
    "        (objects, fixed) = retrieve_concepts_sampling(target_objects)\n",
    "        # add one such that zero becomes an empty attribute for the calculation (_)\n",
    "        objects = objects + 1\n",
    "        concepts = torch.from_numpy(objects * (np.array(fixed)))\n",
    "\n",
    "        # concrete/specific concepts: where all attributes are fixed\n",
    "        concepts_specific = torch.tensor(\n",
    "            objects[torch.sum(torch.from_numpy(fixed), dim=1) == n_attributes[d]])\n",
    "        messages_specific = messages[torch.sum(torch.from_numpy(fixed), dim=1) == n_attributes[d]]\n",
    "\n",
    "        # generic concepts: where only one attribute is fixed\n",
    "        concepts_generic = torch.tensor(\n",
    "            objects[torch.sum(torch.from_numpy(fixed), dim=1) == 1])\n",
    "        messages_generic = messages[torch.sum(torch.from_numpy(fixed), dim=1) == 1]\n",
    "        \n",
    "        posdis_specific = Disent.posdis(concepts_specific, messages_specific)\n",
    "        bosdis_specific = Disent.bosdis(concepts_specific, messages_specific, vocab_size)\n",
    "\n",
    "        posdis_generic = Disent.posdis(concepts_generic, messages_generic)\n",
    "        bosdis_generic = Disent.bosdis(concepts_generic, messages_generic, vocab_size)\n",
    "        \n",
    "        posdis = Disent.posdis(torch.from_numpy(objects), messages)\n",
    "        bosdis = Disent.bosdis(torch.from_numpy(objects), messages, vocab_size)\n",
    "        \n",
    "        posdis_bosdis['posdis_specific'] = posdis_specific\n",
    "        posdis_bosdis['bosdis_specific'] = bosdis_specific\n",
    "        posdis_bosdis['posdis_generic'] = posdis_generic\n",
    "        posdis_bosdis['bosdis_generic'] = bosdis_generic\n",
    "        posdis_bosdis['posdis'] = posdis\n",
    "        posdis_bosdis['bosdis'] = bosdis\n",
    "\n",
    "        print(posdis_bosdis)\n",
    "    \n",
    "        pickle.dump(posdis_bosdis, open(path_to_run + \"posdis_bosdis.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Posdis and bosdis concept x context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T16:21:47.222765Z",
     "start_time": "2024-02-28T16:21:29.736013Z"
    }
   },
   "outputs": [],
   "source": [
    "# bosdis concept x context\n",
    "for d in range(len(datasets)):\n",
    "\n",
    "    vs_factor = int(paths[d][-2])\n",
    "    vocab_size = (n_values[d] + 1) * vs_factor + 1\n",
    "    \n",
    "    for run in range(5):\n",
    "\n",
    "        path_to_run = paths[d] + '/' + str(setting) +'/' + str(run) + '/' \n",
    "        path_to_interaction = (path_to_run + 'interactions/train/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "        interaction = torch.load(path_to_interaction)\n",
    "\n",
    "        attributes = n_attributes[d]\n",
    "        values = n_values[d]\n",
    "        scores = bosdis(interaction, attributes, values, vocab_size)\n",
    "\n",
    "        pickle.dump(scores, open(path_to_run + 'bosdis_scores.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T16:22:09.615176Z",
     "start_time": "2024-02-28T16:21:47.223647Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# posdis concept x context\n",
    "for d in range(len(datasets)):\n",
    "\n",
    "    vs_factor = int(paths[d][-2])\n",
    "    vocab_size = (n_values[d] + 1) * vs_factor + 1\n",
    "\n",
    "    for run in range(5):\n",
    "        path_to_run = paths[d] + '/' + str(setting) + '/' + str(run) + '/'\n",
    "        print(path_to_run)\n",
    "        path_to_interaction = (path_to_run + 'interactions/train/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "        #print(path_to_interaction)\n",
    "        interaction = torch.load(path_to_interaction)\n",
    "\n",
    "        attributes = n_attributes[d]\n",
    "        values = n_values[d]\n",
    "        scores = posdis(interaction, attributes, values, vocab_size)\n",
    "\n",
    "        pickle.dump(scores, open(path_to_run + 'posdis_scores.pkl', 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## co-occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not yet implemented:\n",
    "\n",
    "for d in range(len(datasets)):\n",
    "    \n",
    "    vs_factor = int(paths[d][-2])\n",
    "    \n",
    "    for run in range(5): \n",
    "        \n",
    "        path_to_run = paths[d] + str(setting) +'/' + str(run) + '/'\n",
    "        path_to_interaction = (path_to_run + 'interactions/train/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "        interaction = torch.load(path_to_interaction)\n",
    "\n",
    "        attributes = n_attributes[d]\n",
    "        values = n_values[d]\n",
    "        \n",
    "        scores = cooccurrence_per_hierarchy_level(interaction, attributes, values, vs_factor)\n",
    "\n",
    "        print(scores)\n",
    "        \n",
    "        pickle.dump(scores, open(path_to_run + 'normalized_cooccurrence.pkl', 'wb'))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
