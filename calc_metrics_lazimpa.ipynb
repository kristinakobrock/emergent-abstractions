{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T14:07:31.005473Z",
     "start_time": "2024-03-14T14:07:31.002334Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from utils.analysis_from_interaction import *\n",
    "from egg.core.language_analysis import Disent\n",
    "from language_analysis_local import TopographicSimilarityConceptLevel, encode_target_concepts_for_topsim\n",
    "from utils.analysis_tools_lazimpa import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculate metrics from stored interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T14:07:35.486572Z",
     "start_time": "2024-03-14T14:07:35.482298Z"
    }
   },
   "outputs": [],
   "source": [
    "datasets = ('(3,4)', '(3,8)', '(4,4)')\n",
    "n_attributes = (3, 3, 4)\n",
    "n_values = (4, 8, 4)\n",
    "n_epochs = 300\n",
    "paths = ['results/' + d + '_game_size_10_vsf_3/' for d in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-06T14:06:31.077419Z",
     "start_time": "2024-03-06T14:06:31.064730Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datasets = ('(4,4)',)\n",
    "n_attributes = (4,)\n",
    "n_values = (4,)\n",
    "n_epochs = 100\n",
    "paths = ['results/' + d + '_game_size_10_vsf_3/' for d in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['(3,4)',]\n",
    "n_values = [4,]\n",
    "n_attributes = [3,]\n",
    "n_epochs = 300\n",
    "paths = ['results/' + d + '_game_size_10_vsf_3/' for d in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lazimpa_context_aware'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_unaware = False # whether original or context_unaware simulations are evaluated\n",
    "lazy = True # whether the lazy agent is used\n",
    "impatient = True\n",
    "setting = \"\"\n",
    "pickle_data = True\n",
    "if not lazy:\n",
    "    start = 'impatience_' if impatient else ''\n",
    "    if context_unaware:\n",
    "        setting = start + 'context_unaware'\n",
    "    else:\n",
    "        setting = start + 'context_aware'\n",
    "        if impatient == False:\n",
    "            setting = 'standard'\n",
    "else:\n",
    "    start = 'lazimpa' if impatient else 'lazy'\n",
    "    if context_unaware:\n",
    "        setting = start + '_context_unaware'\n",
    "    else:\n",
    "        setting = start + '_context_aware'\n",
    "setting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## entropy scores: MI, effectiveness, efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-06T14:06:37.646673Z",
     "start_time": "2024-03-06T14:06:33.540807Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/(3,4)_game_size_10_vsf_3//lazimpa_context_aware/0/interactions/train/epoch_300/interaction_gpu0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Documents\\GitHub\\emergent-abstractions\\utils\\analysis_from_interaction.py:315: RuntimeWarning: invalid value encountered in divide\n",
      "  (m_entropy_concept_x_context + c_entropy_concept_x_context - joint_entropy_concept_x_context)\n",
      "d:\\Documents\\GitHub\\emergent-abstractions\\utils\\analysis_from_interaction.py:324: RuntimeWarning: invalid value encountered in divide\n",
      "  normalized_effectiveness_conc_x_cont = ((joint_entropy_concept_x_context - m_entropy_concept_x_context)\n",
      "d:\\Documents\\GitHub\\emergent-abstractions\\utils\\analysis_from_interaction.py:331: RuntimeWarning: invalid value encountered in divide\n",
      "  normalized_consistency_conc_x_cont = (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/(3,4)_game_size_10_vsf_3//lazimpa_context_aware/1/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(3,4)_game_size_10_vsf_3//lazimpa_context_aware/2/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(3,4)_game_size_10_vsf_3//lazimpa_context_aware/3/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(3,4)_game_size_10_vsf_3//lazimpa_context_aware/4/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(3,8)_game_size_10_vsf_3//lazimpa_context_aware/0/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(3,8)_game_size_10_vsf_3//lazimpa_context_aware/1/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(3,8)_game_size_10_vsf_3//lazimpa_context_aware/2/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(3,8)_game_size_10_vsf_3//lazimpa_context_aware/3/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(3,8)_game_size_10_vsf_3//lazimpa_context_aware/4/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(4,4)_game_size_10_vsf_3//lazimpa_context_aware/0/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(4,4)_game_size_10_vsf_3//lazimpa_context_aware/1/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(4,4)_game_size_10_vsf_3//lazimpa_context_aware/2/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(4,4)_game_size_10_vsf_3//lazimpa_context_aware/3/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(4,4)_game_size_10_vsf_3//lazimpa_context_aware/4/interactions/train/epoch_300/interaction_gpu0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'normalized_mutual_info': 0.5646424659616172,\n",
       " 'normalized_mutual_info_hierarchical': array([0.9412313 , 0.71581218, 0.61050991, 0.56984363]),\n",
       " 'normalized_mutual_info_context_dep': array([0.60728343, 0.64302973, 0.69433659, 0.73332883]),\n",
       " 'normalized_mutual_info_concept_x_context': array([0.9412313 ,        nan,        nan,        nan, 0.73845603,\n",
       "        0.75726969,        nan,        nan, 0.64400332, 0.67973626,\n",
       "        0.72967362,        nan, 0.62280925, 0.65584262, 0.70397423,\n",
       "        0.73332883]),\n",
       " 'effectiveness': 0.5327886972087245,\n",
       " 'effectiveness_hierarchical': array([0.95796588, 0.7092366 , 0.60438355, 0.58895723]),\n",
       " 'effectiveness_context_dep': array([0.52052878, 0.58264702, 0.68003261, 0.7767193 ]),\n",
       " 'effectiveness_concept_x_context': array([0.95796588,        nan,        nan,        nan, 0.69333176,\n",
       "        0.75908013,        nan,        nan, 0.58151209, 0.65248153,\n",
       "        0.74343608,        nan, 0.57371813, 0.62992165, 0.72519545,\n",
       "        0.7767193 ]),\n",
       " 'consistency': 0.6005473101182175,\n",
       " 'consistency_hierarchical': array([0.92507135, 0.72251083, 0.61676173, 0.55193163]),\n",
       " 'consistency_context_dep': array([0.72873988, 0.71737498, 0.70925525, 0.69452978]),\n",
       " 'consistency_concept_x_context': array([0.92507135,        nan,        nan,        nan, 0.78986285,\n",
       "        0.75546787,        nan,        nan, 0.72154269, 0.70936714,\n",
       "        0.71641144,        nan, 0.68108762, 0.68398841, 0.68395968,\n",
       "        0.69452978])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for d in range(len(datasets)):\n",
    "    \n",
    "    for run in range(5):\n",
    "\n",
    "        path_to_run = paths[d] + str(setting) +'/' + str(run) + '/' \n",
    "        interaction = load_interaction(paths[d],setting,run,n_epochs)\n",
    "\n",
    "        attributes = n_attributes[d]\n",
    "        values = n_values[d]\n",
    "        scores = information_scores(interaction, attributes, values, normalizer=\"arithmetic\")\n",
    "\n",
    "        if pickle_data:\n",
    "            pickle.dump(scores, open(path_to_run + 'entropy_scores.pkl', 'wb'))\n",
    "scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  message length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T16:14:53.162856Z",
     "start_time": "2024-02-28T16:14:51.541390Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/(3,4)_game_size_10_vsf_3//lazimpa_context_aware/0/interactions/train/epoch_300/interaction_gpu0\n",
      "message length tensor([1, 1, 1,  ..., 1, 2, 1])\n",
      "hierarchical [1.402, 1.076, 1.122]\n",
      "results/(3,4)_game_size_10_vsf_3//lazimpa_context_aware/1/interactions/train/epoch_300/interaction_gpu0\n",
      "message length tensor([1, 1, 1,  ..., 1, 1, 0])\n",
      "hierarchical [1.465, 1.296, 1.183]\n",
      "results/(3,4)_game_size_10_vsf_3//lazimpa_context_aware/2/interactions/train/epoch_300/interaction_gpu0\n",
      "message length tensor([0, 1, 1,  ..., 1, 1, 2])\n",
      "hierarchical [1.042, 1.08, 1.085]\n",
      "results/(3,4)_game_size_10_vsf_3//lazimpa_context_aware/3/interactions/train/epoch_300/interaction_gpu0\n",
      "message length tensor([1, 2, 1,  ..., 1, 0, 1])\n",
      "hierarchical [1.036, 1.077, 1.03]\n",
      "results/(3,4)_game_size_10_vsf_3//lazimpa_context_aware/4/interactions/train/epoch_300/interaction_gpu0\n",
      "message length tensor([1, 1, 1,  ..., 1, 1, 1])\n",
      "hierarchical [1.441, 1.197, 1.203]\n",
      "results/(3,8)_game_size_10_vsf_3//lazimpa_context_aware/0/interactions/train/epoch_300/interaction_gpu0\n",
      "message length tensor([1, 2, 2,  ..., 2, 2, 2])\n",
      "hierarchical [1.8, 1.542, 1.467]\n",
      "results/(3,8)_game_size_10_vsf_3//lazimpa_context_aware/1/interactions/train/epoch_300/interaction_gpu0\n",
      "message length tensor([2, 0, 2,  ..., 2, 2, 1])\n",
      "hierarchical [1.741, 1.285, 1.376]\n",
      "results/(3,8)_game_size_10_vsf_3//lazimpa_context_aware/2/interactions/train/epoch_300/interaction_gpu0\n",
      "message length tensor([2, 1, 2,  ..., 2, 2, 2])\n",
      "hierarchical [1.88, 1.709, 1.681]\n",
      "results/(3,8)_game_size_10_vsf_3//lazimpa_context_aware/3/interactions/train/epoch_300/interaction_gpu0\n",
      "message length tensor([1, 0, 1,  ..., 1, 2, 1])\n",
      "hierarchical [2.261, 1.724, 1.414]\n",
      "results/(3,8)_game_size_10_vsf_3//lazimpa_context_aware/4/interactions/train/epoch_300/interaction_gpu0\n",
      "message length tensor([1, 1, 1,  ..., 1, 1, 2])\n",
      "hierarchical [1.584, 1.344, 1.338]\n",
      "results/(4,4)_game_size_10_vsf_3//lazimpa_context_aware/0/interactions/train/epoch_300/interaction_gpu0\n",
      "message length tensor([0, 0, 0,  ..., 1, 1, 1])\n",
      "hierarchical [0.808, 0.988, 1.055, 1.083]\n",
      "results/(4,4)_game_size_10_vsf_3//lazimpa_context_aware/1/interactions/train/epoch_300/interaction_gpu0\n",
      "message length tensor([1, 1, 2,  ..., 2, 2, 2])\n",
      "hierarchical [1.084, 1.142, 1.084, 1.078]\n",
      "results/(4,4)_game_size_10_vsf_3//lazimpa_context_aware/2/interactions/train/epoch_300/interaction_gpu0\n",
      "message length tensor([0, 1, 1,  ..., 2, 2, 1])\n",
      "hierarchical [1.01, 1.134, 1.169, 1.215]\n",
      "results/(4,4)_game_size_10_vsf_3//lazimpa_context_aware/3/interactions/train/epoch_300/interaction_gpu0\n",
      "message length tensor([1, 1, 1,  ..., 1, 1, 0])\n",
      "hierarchical [1.022, 1.017, 1.062, 1.095]\n",
      "results/(4,4)_game_size_10_vsf_3//lazimpa_context_aware/4/interactions/train/epoch_300/interaction_gpu0\n",
      "message length tensor([1, 1, 1,  ..., 1, 1, 2])\n",
      "hierarchical [0.895, 1.011, 1.046, 1.065]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.895, 1.011, 1.046, 1.065]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we evaluated message length per hierarchy level after training but \n",
    "# you can also use the HierarchicalMessageLength callback and store the results \n",
    "# TODO: Message length results look weird, needs to be fixed!\n",
    "\n",
    "for d in range(len(datasets)):\n",
    "    \n",
    "    for run in range(5):\n",
    "        \n",
    "        path_to_run = paths[d] + str(setting) +'/' + str(run) + '/' \n",
    "        interaction = load_interaction(paths[d],setting,run,n_epochs)\n",
    "\n",
    "        attributes = n_attributes[d]\n",
    "        values = n_values[d]\n",
    "        scores = message_length_per_hierarchy_level(interaction, attributes)\n",
    "        \n",
    "        if pickle_data:\n",
    "            pickle.dump(scores, open(path_to_run + 'message_length_hierarchical.pkl', 'wb'))\n",
    "scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  symbol redundancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T16:14:57.892753Z",
     "start_time": "2024-02-28T16:14:53.164028Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/(3,4)_game_size_10_vsf_3//lazimpa_context_aware/0/interactions/train/epoch_300/interaction_gpu0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Documents\\GitHub\\emergent-abstractions\\utils\\analysis_from_interaction.py:446: RuntimeWarning: invalid value encountered in divide\n",
      "  return symbol_frequency / att_val_frequency, mutual_information\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/(3,4)_game_size_10_vsf_3//lazimpa_context_aware/1/interactions/train/epoch_300/interaction_gpu0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Documents\\GitHub\\emergent-abstractions\\utils\\analysis_from_interaction.py:446: RuntimeWarning: invalid value encountered in divide\n",
      "  return symbol_frequency / att_val_frequency, mutual_information\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/(3,4)_game_size_10_vsf_3//lazimpa_context_aware/2/interactions/train/epoch_300/interaction_gpu0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Documents\\GitHub\\emergent-abstractions\\utils\\analysis_from_interaction.py:446: RuntimeWarning: invalid value encountered in divide\n",
      "  return symbol_frequency / att_val_frequency, mutual_information\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/(3,4)_game_size_10_vsf_3//lazimpa_context_aware/3/interactions/train/epoch_300/interaction_gpu0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Documents\\GitHub\\emergent-abstractions\\utils\\analysis_from_interaction.py:446: RuntimeWarning: invalid value encountered in divide\n",
      "  return symbol_frequency / att_val_frequency, mutual_information\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/(3,4)_game_size_10_vsf_3//lazimpa_context_aware/4/interactions/train/epoch_300/interaction_gpu0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Documents\\GitHub\\emergent-abstractions\\utils\\analysis_from_interaction.py:446: RuntimeWarning: invalid value encountered in divide\n",
      "  return symbol_frequency / att_val_frequency, mutual_information\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/(3,8)_game_size_10_vsf_3//lazimpa_context_aware/0/interactions/train/epoch_300/interaction_gpu0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Documents\\GitHub\\emergent-abstractions\\utils\\analysis_from_interaction.py:446: RuntimeWarning: invalid value encountered in divide\n",
      "  return symbol_frequency / att_val_frequency, mutual_information\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/(3,8)_game_size_10_vsf_3//lazimpa_context_aware/1/interactions/train/epoch_300/interaction_gpu0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Documents\\GitHub\\emergent-abstractions\\utils\\analysis_from_interaction.py:446: RuntimeWarning: invalid value encountered in divide\n",
      "  return symbol_frequency / att_val_frequency, mutual_information\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/(3,8)_game_size_10_vsf_3//lazimpa_context_aware/2/interactions/train/epoch_300/interaction_gpu0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Documents\\GitHub\\emergent-abstractions\\utils\\analysis_from_interaction.py:446: RuntimeWarning: invalid value encountered in divide\n",
      "  return symbol_frequency / att_val_frequency, mutual_information\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/(3,8)_game_size_10_vsf_3//lazimpa_context_aware/3/interactions/train/epoch_300/interaction_gpu0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Documents\\GitHub\\emergent-abstractions\\utils\\analysis_from_interaction.py:446: RuntimeWarning: invalid value encountered in divide\n",
      "  return symbol_frequency / att_val_frequency, mutual_information\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/(3,8)_game_size_10_vsf_3//lazimpa_context_aware/4/interactions/train/epoch_300/interaction_gpu0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Documents\\GitHub\\emergent-abstractions\\utils\\analysis_from_interaction.py:446: RuntimeWarning: invalid value encountered in divide\n",
      "  return symbol_frequency / att_val_frequency, mutual_information\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/(4,4)_game_size_10_vsf_3//lazimpa_context_aware/0/interactions/train/epoch_300/interaction_gpu0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Documents\\GitHub\\emergent-abstractions\\utils\\analysis_from_interaction.py:446: RuntimeWarning: invalid value encountered in divide\n",
      "  return symbol_frequency / att_val_frequency, mutual_information\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/(4,4)_game_size_10_vsf_3//lazimpa_context_aware/1/interactions/train/epoch_300/interaction_gpu0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Documents\\GitHub\\emergent-abstractions\\utils\\analysis_from_interaction.py:446: RuntimeWarning: invalid value encountered in divide\n",
      "  return symbol_frequency / att_val_frequency, mutual_information\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/(4,4)_game_size_10_vsf_3//lazimpa_context_aware/2/interactions/train/epoch_300/interaction_gpu0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Documents\\GitHub\\emergent-abstractions\\utils\\analysis_from_interaction.py:446: RuntimeWarning: invalid value encountered in divide\n",
      "  return symbol_frequency / att_val_frequency, mutual_information\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/(4,4)_game_size_10_vsf_3//lazimpa_context_aware/3/interactions/train/epoch_300/interaction_gpu0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Documents\\GitHub\\emergent-abstractions\\utils\\analysis_from_interaction.py:446: RuntimeWarning: invalid value encountered in divide\n",
      "  return symbol_frequency / att_val_frequency, mutual_information\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/(4,4)_game_size_10_vsf_3//lazimpa_context_aware/4/interactions/train/epoch_300/interaction_gpu0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Documents\\GitHub\\emergent-abstractions\\utils\\analysis_from_interaction.py:446: RuntimeWarning: invalid value encountered in divide\n",
      "  return symbol_frequency / att_val_frequency, mutual_information\n"
     ]
    }
   ],
   "source": [
    "for d in range(len(datasets)):\n",
    "    \n",
    "    attributes = n_attributes[d]\n",
    "    values = n_values[d]\n",
    "    vs_factor = int(paths[d][-2])\n",
    "    vocab_size = (n_values[d] + 1) * vs_factor + 1\n",
    "    \n",
    "    for run in range(5):\n",
    "        \n",
    "        path_to_run = paths[d] + str(setting) +'/' + str(run) + '/'\n",
    "        #symbol_f = np.load(path_to_run + 'symbols_pernsame.npy')\n",
    "        interaction = load_interaction(paths[d],setting,run,n_epochs)\n",
    "        redundancy, MI = symbol_frequency(interaction, attributes, values, vocab_size)\n",
    "        \n",
    "        scores = {'symbol_redundancy': redundancy, 'MI_symbol-attribute_value': MI}\n",
    "        \n",
    "        if pickle_data:\n",
    "            pickle.dump(scores, open(path_to_run + 'symbol_redundancy.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ZLA significance scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/(3,4)_game_size_10_vsf_3//lazimpa_context_aware/0/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(3,4)_game_size_10_vsf_3//lazimpa_context_aware/1/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(3,4)_game_size_10_vsf_3//lazimpa_context_aware/2/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(3,4)_game_size_10_vsf_3//lazimpa_context_aware/3/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(3,4)_game_size_10_vsf_3//lazimpa_context_aware/4/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(3,8)_game_size_10_vsf_3//lazimpa_context_aware/0/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(3,8)_game_size_10_vsf_3//lazimpa_context_aware/1/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(3,8)_game_size_10_vsf_3//lazimpa_context_aware/2/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(3,8)_game_size_10_vsf_3//lazimpa_context_aware/3/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(3,8)_game_size_10_vsf_3//lazimpa_context_aware/4/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(4,4)_game_size_10_vsf_3//lazimpa_context_aware/0/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(4,4)_game_size_10_vsf_3//lazimpa_context_aware/1/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(4,4)_game_size_10_vsf_3//lazimpa_context_aware/2/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(4,4)_game_size_10_vsf_3//lazimpa_context_aware/3/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(4,4)_game_size_10_vsf_3//lazimpa_context_aware/4/interactions/train/epoch_300/interaction_gpu0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_message_length': tensor(2.0389),\n",
       " 'mean_weighted_message_length': tensor(1.0512),\n",
       " 'p_zla': tensor(0.)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for d in range(len(datasets)):\n",
    "    \n",
    "    \n",
    "    for run in range(5):\n",
    "\n",
    "        path_to_run = paths[d] + str(setting) +'/' + str(run) + '/' \n",
    "        interaction = load_interaction(paths[d],setting,run,n_epochs)\n",
    "\n",
    "        scores = ZLA_significance_score(interaction,num_permutations=1000,remove_after_eos=True)\n",
    "\n",
    "        if pickle_data:\n",
    "            pickle.dump(scores, open(path_to_run + 'ZLA_significance.pkl', 'wb'))\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symbol Informativeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/(3,4)_game_size_10_vsf_3//lazimpa_context_aware/0/interactions/train/epoch_300/interaction_gpu0\n",
      "\n",
      " Max (normal) switch:  {'positional_encoding': tensor([0.9693, 0.9582, 0.7778]), 'effective_length': tensor(1.0817), 'information_density': tensor(0.9660)}\n",
      "\n",
      "Unimportant switch:  {'positional_encoding': tensor([0.0489, 0.0070, 0.0000]), 'effective_length': tensor(0.0443), 'information_density': tensor(0.0395)}\n",
      "Number of important symbols:  tensor(78)\n"
     ]
    }
   ],
   "source": [
    "# Test if change of two unimportant values, without changing the actual symbol has an effect\n",
    "interaction = load_interaction(paths[0],setting,0,n_epochs)\n",
    "attributes = n_attributes[0]\n",
    "values = n_values[0]\n",
    "listener,_,_ = load_listener(paths[0],setting,0,attributes, values,context_unaware)\n",
    "eos_token = 0.0\n",
    "\n",
    "# symbol informativeness\n",
    "messages = interaction.message\n",
    "num_messages, message_length, vocab_size = messages.shape\n",
    "\n",
    "# values to be switched\n",
    "max_values, original_messages = messages.max(dim=-1)\n",
    "\n",
    "eos_mask_total = eos_token != original_messages\n",
    "\n",
    "# only values before eos, to switch\n",
    "eos_excluded_cumulative = torch.cat([ eos_mask_total[:,:i+1].sum(dim=1).unsqueeze(1) == i+1 for i in range(message_length) ],dim=1)\n",
    "\n",
    "# until eos, evaluating changes\n",
    "eos_included_cumualtive = torch.cat([torch.ones_like(eos_excluded_cumulative[:,-1]).unsqueeze(1).bool(), eos_excluded_cumulative[:,:-1]],dim=1)\n",
    "\n",
    "# Prediction for original messages, exclude prediction for values after eos\n",
    "prediction = (listener(messages,interaction.receiver_input) > 0).float()\n",
    "empty_prediction = torch.zeros_like(prediction)\n",
    "prediction = torch.where(eos_included_cumualtive.unsqueeze(-1),prediction,empty_prediction)\n",
    "\n",
    "Lambda_m_k_list = []\n",
    "\n",
    "for i in range(message_length-1): # index = -1 is always eos and not changed. \n",
    "\n",
    "    # pick random new symbol index to switch with\n",
    "    switch_index = torch.randint(1,vocab_size,(num_messages,))\n",
    "\n",
    "    # make sure switch_index != original_messages\n",
    "    same_indices = (switch_index == original_messages[:,i])\n",
    "    while same_indices.any():\n",
    "        switch_index[same_indices] = torch.randint(1, vocab_size, (same_indices.sum().item(),))\n",
    "        same_indices = (switch_index == original_messages[:,i])\n",
    "\n",
    "    # values to switch with\n",
    "    new_value = torch.gather(messages[:,i,:],-1,switch_index.unsqueeze(-1)).squeeze()\n",
    "\n",
    "    # pick random new symbol index to switch with #2 for switching in background\n",
    "    switch_index2 = torch.randint(1,vocab_size,(num_messages,))\n",
    "\n",
    "    # make sure switch_index != original_messages and != switch_index\n",
    "    same_indices = (switch_index2 == original_messages[:,i]) + (switch_index2 == switch_index)\n",
    "    while same_indices.any():\n",
    "        switch_index2[same_indices] = torch.randint(1, vocab_size, (same_indices.sum().item(),))\n",
    "        same_indices = (switch_index2 == original_messages[:,i]) + (switch_index2 == switch_index)\n",
    "\n",
    "    # values to switch with\n",
    "    new_value2 = torch.gather(messages[:,i,:],-1,switch_index2.unsqueeze(-1)).squeeze()\n",
    "\n",
    "    messages_manipulated = messages.clone()\n",
    "\n",
    "    # if eosed, take old value in so no switch (with 2 not max)\n",
    "    new_value_without_eos = torch.where(eos_excluded_cumulative[:,i].squeeze(), new_value,new_value2)\n",
    "    new_value2_without_eos = torch.where(eos_excluded_cumulative[:,i].squeeze(), new_value2,new_value)\n",
    "\n",
    "    # switch value i of all messages to new symbol # not max\n",
    "    messages_manipulated[:,i].scatter_(-1,switch_index2.unsqueeze(-1),new_value_without_eos.unsqueeze(-1))\n",
    "    messages_manipulated[:,i].scatter_(-1,switch_index.unsqueeze(-1),new_value2_without_eos.unsqueeze(-1))\n",
    "\n",
    "    prediction_manipulated = (listener(messages_manipulated,interaction.receiver_input) > 0).float() # bigger 0 means thinks its a concept object\n",
    "    prediction_manipulated = torch.where(eos_included_cumualtive.unsqueeze(-1),prediction_manipulated,empty_prediction)\n",
    "\n",
    "    # compare whether same classification not same values\n",
    "    # only include predictions for symbols that are not eosed yet.\n",
    "    Lambda_m_k_position = (prediction != prediction_manipulated).sum(dim=(1,2)).unsqueeze(1).bool() # (num_messages, 1 )\n",
    "    Lambda_m_k_list.append(Lambda_m_k_position)\n",
    "\n",
    "Lambda_m_k = torch.cat(Lambda_m_k_list,dim=1)\n",
    "eos_mask = eos_excluded_cumulative[:,:-1] # both should be the same shape\n",
    "\n",
    "scores = information_analysis(listener,interaction,eos_token=0.0)\n",
    "print(\"\\n Max (normal) switch: \", scores)\n",
    "\n",
    "return_dict = {\n",
    "    \"positional_encoding\" : positional_encoding(Lambda_m_k, eos_mask),\n",
    "    \"effective_length\" : effective_length(Lambda_m_k, eos_mask),\n",
    "    \"information_density\" : information_density(Lambda_m_k,eos_mask)\n",
    "}\n",
    "print(\"\\nUnimportant switch: \", return_dict)\n",
    "print(\"Number of important symbols: \", Lambda_m_k.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/(3,4)_game_size_10_vsf_3//lazimpa_context_aware/0/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(3,4)_game_size_10_vsf_3//lazimpa_context_aware/1/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(3,4)_game_size_10_vsf_3//lazimpa_context_aware/2/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(3,4)_game_size_10_vsf_3//lazimpa_context_aware/3/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(3,4)_game_size_10_vsf_3//lazimpa_context_aware/4/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(3,8)_game_size_10_vsf_3//lazimpa_context_aware/0/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(3,8)_game_size_10_vsf_3//lazimpa_context_aware/1/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(3,8)_game_size_10_vsf_3//lazimpa_context_aware/2/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(3,8)_game_size_10_vsf_3//lazimpa_context_aware/3/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(3,8)_game_size_10_vsf_3//lazimpa_context_aware/4/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(4,4)_game_size_10_vsf_3//lazimpa_context_aware/0/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(4,4)_game_size_10_vsf_3//lazimpa_context_aware/1/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(4,4)_game_size_10_vsf_3//lazimpa_context_aware/2/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(4,4)_game_size_10_vsf_3//lazimpa_context_aware/3/interactions/train/epoch_300/interaction_gpu0\n",
      "results/(4,4)_game_size_10_vsf_3//lazimpa_context_aware/4/interactions/train/epoch_300/interaction_gpu0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'positional_encoding': tensor([0.9172, 0.9225, 0.8148, 1.0000]),\n",
       " 'effective_length': tensor(0.9650),\n",
       " 'information_density': tensor(0.9180)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normal symbol information analysis\n",
    "for d in range(len(datasets)):\n",
    "    \n",
    "    for run in range(5):\n",
    "\n",
    "        path_to_run = paths[d] + str(setting) +'/' + str(run) + '/' \n",
    "        interaction = load_interaction(paths[d],setting,run,n_epochs)\n",
    "\n",
    "        attributes = n_attributes[d]\n",
    "        values = n_values[d]\n",
    "\n",
    "        listener,_,_ = load_listener(paths[d],setting,run,attributes, values,context_unaware)\n",
    "\n",
    "        scores = information_analysis(listener,interaction,eos_token=0.0)\n",
    "\n",
    "        if pickle_data:\n",
    "            pickle.dump(scores, open(path_to_run + 'symbol_informativeness.pkl', 'wb'))\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/(3,4)_game_size_10_vsf_3//lazimpa_context_aware/4/interactions/train/epoch_300/interaction_gpu0\n",
      "tensor([1., 1., 1.,  ..., 1., 1., 1.])\n",
      "tensor([1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000])\n",
      "tensor([1.7881e-05, 1.1921e-06, 0.0000e+00,  ..., 8.3447e-07, 2.2888e-04,\n",
      "        1.9958e-03])\n",
      "tensor([1.7878e-05, 1.1911e-06, 0.0000e+00,  ..., 6.4132e-07, 2.2738e-04,\n",
      "        1.9958e-03])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([2.0000, 2.0000, 2.0000,  ..., 2.0000, 2.0005, 2.0040])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# todo debugging only\n",
    "interaction = load_interaction(paths[d],setting,run,n_epochs)\n",
    "listener,_,_ = load_listener(paths[d],setting,run,attributes, values,context_unaware)\n",
    "message = interaction.message\n",
    "receiver_output = listener(message,interaction.receiver_input)\n",
    "\n",
    "impatience = True\n",
    "\n",
    "not_eosed_before = torch.ones(receiver_output.size(0))\n",
    "expected_length = 0.0 # final includes eos here\n",
    "z = 0.0\n",
    "\n",
    "for step in range(receiver_output.size(1)):\n",
    "    eos_mask = message[:, step, 0]  # always eos == 0\n",
    "    if impatience:\n",
    "        add_mask = not_eosed_before\n",
    "    else:\n",
    "        add_mask = eos_mask * not_eosed_before\n",
    "    z += add_mask\n",
    "    expected_length += add_mask.detach() * (1.0 + step)\n",
    "    print(not_eosed_before)\n",
    "    not_eosed_before = not_eosed_before * (1.0 - eos_mask)\n",
    "\n",
    "expected_length += (step + 1) * not_eosed_before\n",
    "z += not_eosed_before\n",
    "assert z.allclose(\n",
    "            z\n",
    "        ), f\"lost probability mass, {z.min()}, {z.max()}\"\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis for attribute visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  compositionality scores: topsim, posdis, bosdis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### topsim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-06T18:20:39.525914Z",
     "start_time": "2024-03-06T14:09:08.937473Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset (3,4) run 0\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.1993179436851956, 'topsim_specific_train': 0.22289012375986098, 'topsim_generic_train': 0.3070700850045201, 'topsim_val': 0.2242431716732342, 'topsim_specific_val': 0.27226513675041514, 'topsim_generic_val': 0.3135998635217133}\n",
      "dataset (3,4) run 1\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.18503259651393075, 'topsim_specific_train': 0.20354013643571475, 'topsim_generic_train': 0.38178840983386914, 'topsim_val': 0.23233058627732506, 'topsim_specific_val': 0.2619156159893861, 'topsim_generic_val': 0.2865912960301693}\n",
      "dataset (3,4) run 2\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.22784312129289738, 'topsim_specific_train': 0.2348482529724461, 'topsim_generic_train': 0.3289677946979688, 'topsim_val': 0.25799971965965923, 'topsim_specific_val': 0.2647576115695098, 'topsim_generic_val': 0.1538921964646194}\n",
      "dataset (3,4) run 3\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.2013641069838806, 'topsim_specific_train': 0.189647727528321, 'topsim_generic_train': 0.26879750639425487, 'topsim_val': 0.2529031028693822, 'topsim_specific_val': 0.2491444499601285, 'topsim_generic_val': 0.3341198730145754}\n",
      "dataset (3,4) run 4\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.1637716525802612, 'topsim_specific_train': 0.1639435754833107, 'topsim_generic_train': 0.6047689500252655, 'topsim_val': 0.21451656516607756, 'topsim_specific_val': 0.18750319635139684, 'topsim_generic_val': 0.38136687953570425}\n",
      "dataset (3,8) run 0\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.09885580509243902, 'topsim_specific_train': 0.0986750078788813, 'topsim_generic_train': 0.3814775375551397, 'topsim_val': 0.10807155810033624, 'topsim_specific_val': 0.11255913481638628, 'topsim_generic_val': 0.3197844315184598}\n",
      "dataset (3,8) run 1\n",
      "... topsim computed\n",
      "... topsim computed\n",
      "{'topsim_train': 0.09603808260969383, 'topsim_specific_train': 0.09985320533730563, 'topsim_generic_train': 0.17225845701412054, 'topsim_val': 0.11895573987057216, 'topsim_specific_val': 0.117281195775314, 'topsim_generic_val': 0.16895025455367335}\n",
      "dataset (3,8) run 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 62\u001b[0m\n\u001b[0;32m     51\u001b[0m encoded_input \u001b[38;5;241m=\u001b[39m encode_target_concepts_for_topsim(sender_input)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# randomly take samples when more than 5000 samples are available\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# if len(encoded_input) > samples: \u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m#     print(\"sampling\")\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m#     topsim = TOPSIM.compute_topsim(sampled_input, sampled_messages)\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m topsim \u001b[38;5;241m=\u001b[39m \u001b[43mTOPSIM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_topsim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded_input\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# default: hausdorff distance for concepts, edit distance for messages\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# if len(concepts_specific) > samples:\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m#     print(\"sampling specific\")\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m#     sample_indices_specific = random.sample(range(len(concepts_specific)), samples)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m#                                             meaning_distance_fn=\"edit\")\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[0;32m     71\u001b[0m topsim_specific \u001b[38;5;241m=\u001b[39m TOPSIM\u001b[38;5;241m.\u001b[39mcompute_topsim(concepts_specific[\u001b[38;5;241m0\u001b[39m:samples], messages_specific[\u001b[38;5;241m0\u001b[39m:samples], \n\u001b[0;32m     72\u001b[0m                                             meaning_distance_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Documents\\GitHub\\emergent-abstractions\\language_analysis_local.py:308\u001b[0m, in \u001b[0;36mTopographicSimilarityConceptLevel.compute_topsim\u001b[1;34m(meanings, messages, meaning_distance_fn, message_distance_fn)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;66;03m# compute distances between concepts\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m slow_meaning_fn:\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;66;03m# Mu & Goodman implement a workaround because scipy pdist() only accepts 2-dimensional arrays\u001b[39;00m\n\u001b[1;32m--> 308\u001b[0m     meaning_dist \u001b[38;5;241m=\u001b[39m \u001b[43mpython_pdist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmeanings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeaning_distance_fn_callable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    310\u001b[0m     meaning_dist \u001b[38;5;241m=\u001b[39m distance\u001b[38;5;241m.\u001b[39mpdist(meanings, meaning_distance_fn_callable)\n",
      "File \u001b[1;32md:\\Documents\\GitHub\\emergent-abstractions\\language_analysis_local.py:211\u001b[0m, in \u001b[0;36mpython_pdist\u001b[1;34m(X, metric, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, m):\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;66;03m# print(\"Xi\", X[i])\u001b[39;00m\n\u001b[0;32m    210\u001b[0m         \u001b[38;5;66;03m# print(\"Xj\", X[j])\u001b[39;00m\n\u001b[1;32m--> 211\u001b[0m         dm[k] \u001b[38;5;241m=\u001b[39m metric(X[i], X[j], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# index at zero because function returns more elements\u001b[39;00m\n\u001b[0;32m    212\u001b[0m         \u001b[38;5;66;03m# print(\"hd\", metric(X[i], X[j]))\u001b[39;00m\n\u001b[0;32m    213\u001b[0m         k \u001b[38;5;241m=\u001b[39m k \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\eosan\\anaconda3\\envs\\emergab\\lib\\site-packages\\scipy\\spatial\\distance.py:409\u001b[0m, in \u001b[0;36mdirected_hausdorff\u001b[1;34m(u, v, seed)\u001b[0m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m u\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m v\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m    407\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mu and v need to have the same \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    408\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumber of columns\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 409\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43m_hausdorff\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirected_hausdorff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m_hausdorff.pyx:38\u001b[0m, in \u001b[0;36mscipy.spatial._hausdorff.directed_hausdorff\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mmtrand.pyx:184\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.__init__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_mt19937.pyx:130\u001b[0m, in \u001b[0;36mnumpy.random._mt19937.MT19937.__init__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\eosan\\anaconda3\\envs\\emergab\\lib\\contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# topsim\n",
    "# although topsim values are stored throughout training if callbacks are verbose, we reevaluate the\n",
    "# final topsim scores with more data points \n",
    "\n",
    "samples = 1000 # maybe shuffle from these because otherwise I just take the first 5,000 (which might not be the best)\n",
    "for d, dataset in enumerate(datasets):\n",
    "    \n",
    "    dim = [n_values[d]]*n_attributes[d]\n",
    "    \n",
    "    for run in range(5):\n",
    "        print(\"dataset\", dataset, \"run\", run)\n",
    "        \n",
    "        topsim_final = {}\n",
    "        path_to_run = paths[d] + str(setting) +'/' + str(run) + '/'\n",
    "        path_to_interaction_train = (path_to_run + 'interactions/train/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "        path_to_interaction_val = (path_to_run + 'interactions/validation/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "        \n",
    "        TOPSIM = TopographicSimilarityConceptLevel(dim, is_gumbel=True)\n",
    "        \n",
    "        for mode in ['train', 'val']:\n",
    "\n",
    "            if mode == 'train':\n",
    "                interaction = torch.load(path_to_interaction_train)\n",
    "            elif mode == 'val':\n",
    "                interaction = torch.load(path_to_interaction_val)\n",
    "                \n",
    "                  \n",
    "            messages = interaction.message.argmax(dim=-1)\n",
    "            sender_input = interaction.sender_input\n",
    "            n_targets = int(sender_input.shape[1]/2)\n",
    "            # get target objects and fixed vectors to re-construct concepts\n",
    "            target_objects = sender_input[:, :n_targets]\n",
    "            target_objects = k_hot_to_attributes(target_objects, n_values[d])\n",
    "            # concepts are defined by a list of target objects (here one sampled target object) and a fixed vector\n",
    "            (objects, fixed) = retrieve_concepts_sampling(target_objects)\n",
    "            # add one such that zero becomes an empty attribute for the calculation (_)\n",
    "            objects = objects + 1\n",
    "            concepts = torch.from_numpy(objects * (np.array(fixed)))\n",
    "            specific_idx = np.where(np.sum(fixed, axis=1)==n_attributes[d])[0]\n",
    "            messages_specific = messages[specific_idx]\n",
    "            concepts_specific = concepts[specific_idx]\n",
    "            \n",
    "            generic_idx = np.where(np.sum(fixed, axis=1)==1)[0]\n",
    "            messages_generic = messages[generic_idx]\n",
    "            concepts_generic = concepts[generic_idx]\n",
    "\n",
    "            messages = [msg.tolist() for msg in messages]\n",
    "            messages_specific = [msg.tolist() for msg in messages_specific]\n",
    "            messages_generic = [msg.tolist() for msg in messages_generic]\n",
    "\n",
    "            encoded_input = encode_target_concepts_for_topsim(sender_input)\n",
    "            # randomly take samples when more than 5000 samples are available\n",
    "            # if len(encoded_input) > samples: \n",
    "            #     print(\"sampling\")\n",
    "            #     sample_indices = random.sample(range(len(encoded_input)), samples)\n",
    "            #     sampled_input = [encoded_input[i] for i in sample_indices]\n",
    "            #     sampled_messages = [messages[i] for i in sample_indices]\n",
    "            #     print(\"start computing\")\n",
    "            #     print(len(sampled_input), len(sampled_input[0]), len(sampled_input[0][0]))\n",
    "            #     topsim = TOPSIM.compute_topsim(sampled_input, sampled_messages)\n",
    "            # else:\n",
    "            topsim = TOPSIM.compute_topsim(encoded_input[0:samples], messages[0:samples]) # default: hausdorff distance for concepts, edit distance for messages\n",
    "            # if len(concepts_specific) > samples:\n",
    "            #     print(\"sampling specific\")\n",
    "            #     sample_indices_specific = random.sample(range(len(concepts_specific)), samples)\n",
    "            #     sampled_input_specific = [concepts_specific[i] for i in sample_indices_specific]\n",
    "            #     sampled_messages_specific = [messages_specific[i] for i in sample_indices_specific]\n",
    "            #     topsim_specific = TOPSIM.compute_topsim(sampled_input_specific, sampled_messages_specific, \n",
    "            #                                             meaning_distance_fn=\"edit\")\n",
    "            # else:\n",
    "            topsim_specific = TOPSIM.compute_topsim(concepts_specific[0:samples], messages_specific[0:samples], \n",
    "                                                        meaning_distance_fn=\"edit\")\n",
    "            \n",
    "            topsim_generic = TOPSIM.compute_topsim(concepts_generic[0:samples], messages_generic[0:samples],\n",
    "                                                   meaning_distance_fn=\"edit\")\n",
    "\n",
    "            print('... topsim computed')\n",
    "\n",
    "            topsim_final['topsim_' + mode] = topsim\n",
    "            topsim_final['topsim_specific_' + mode] = topsim_specific\n",
    "            topsim_final['topsim_generic_' + mode] = topsim_generic\n",
    "    \n",
    "        if pickle_data:\n",
    "            pickle.dump(topsim_final, open(path_to_run +  \"topsim_final.pkl\", \"wb\" ) )\n",
    "        print(topsim_final)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Topsim over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-14T14:07:47.776462Z"
    },
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "for d, dataset in enumerate(datasets):\n",
    "    \n",
    "    for run in range(5):\n",
    "        print(\"dataset\", dataset, \"run\", run)\n",
    "        \n",
    "        path_to_run = paths[d] + str(setting) +'/' + str(run) + '/'\n",
    "        path_to_interaction_train = (path_to_run + 'interactions/train/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "        path_to_interaction_val = (path_to_run + 'interactions/validation/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "        \n",
    "        for mode in ['train', 'val']:\n",
    "\n",
    "            if mode == 'train':\n",
    "                interaction = torch.load(path_to_interaction_train)\n",
    "            elif mode == 'val':\n",
    "                interaction = torch.load(path_to_interaction_val)\n",
    "\n",
    "        messages = interaction.message.argmax(dim=-1)\n",
    "        sender_input = interaction.sender_input\n",
    "        messages = [msg.tolist() for msg in messages]\n",
    "        encoded_input = encode_target_concepts_for_topsim(sender_input)\n",
    "        dim = [n_values[0]] * n_attributes[0]\n",
    "        TOPSIM = TopographicSimilarityConceptLevel(dim, is_gumbel=True)\n",
    "        \n",
    "        samples = 5000\n",
    "        num_batches = len(messages) // samples + (len(messages) % samples > 0)\n",
    "        topsim_over_time = []\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "            messages_batch = messages[i * samples:(i + 1) * samples]\n",
    "            topsim = TOPSIM.compute_topsim(encoded_input[i * samples:(i + 1) * samples], messages_batch)\n",
    "            topsim_over_time.append(topsim)\n",
    "            \n",
    "        if pickle_data:\n",
    "            pickle.dump(topsim_over_time, open(path_to_run +  \"topsim_over_time.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Posdis and Bosdis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T16:16:18.597875Z",
     "start_time": "2024-02-28T16:14:57.896059Z"
    }
   },
   "outputs": [],
   "source": [
    "# use Disent callback from egg\n",
    "\n",
    "for d in range(len(datasets)): \n",
    "    \n",
    "    path = paths[d]\n",
    "    dim = [n_values[d]] * n_attributes[d]\n",
    "    n_features = n_attributes[d] * n_values[d]\n",
    "    vs_factor = int(path[-2])\n",
    "    vocab_size = (n_values[d] + 1) * vs_factor + 1\n",
    "    \n",
    "    print(\"data set\", dim)\n",
    "    \n",
    "    for run in range(5):\n",
    "        \n",
    "        posdis_bosdis = {}\n",
    "    \n",
    "        path_to_run = paths[d] + '/' + str(setting) +'/' + str(run) + '/'\n",
    "        path_to_interaction_train = (path_to_run + 'interactions/train/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "        interaction = torch.load(path_to_interaction_train)\n",
    "        \n",
    "        messages = interaction.message.argmax(dim=-1)\n",
    "        sender_input = interaction.sender_input\n",
    "        n_targets = int(sender_input.shape[1]/2)\n",
    "        # get target objects and fixed vectors to re-construct concepts\n",
    "        target_objects = sender_input[:, :n_targets]\n",
    "        target_objects = k_hot_to_attributes(target_objects, n_values[d])\n",
    "        # concepts are defined by a list of target objects (here one sampled target object) and a fixed vector\n",
    "        (objects, fixed) = retrieve_concepts_sampling(target_objects)\n",
    "        # add one such that zero becomes an empty attribute for the calculation (_)\n",
    "        objects = objects + 1\n",
    "        concepts = torch.from_numpy(objects * (np.array(fixed)))\n",
    "\n",
    "        # concrete/specific concepts: where all attributes are fixed\n",
    "        concepts_specific = torch.tensor(\n",
    "            objects[torch.sum(torch.from_numpy(fixed), dim=1) == n_attributes[d]])\n",
    "        messages_specific = messages[torch.sum(torch.from_numpy(fixed), dim=1) == n_attributes[d]]\n",
    "\n",
    "        # generic concepts: where only one attribute is fixed\n",
    "        concepts_generic = torch.tensor(\n",
    "            objects[torch.sum(torch.from_numpy(fixed), dim=1) == 1])\n",
    "        messages_generic = messages[torch.sum(torch.from_numpy(fixed), dim=1) == 1]\n",
    "        \n",
    "        posdis_specific = Disent.posdis(concepts_specific, messages_specific)\n",
    "        bosdis_specific = Disent.bosdis(concepts_specific, messages_specific, vocab_size)\n",
    "\n",
    "        posdis_generic = Disent.posdis(concepts_generic, messages_generic)\n",
    "        bosdis_generic = Disent.bosdis(concepts_generic, messages_generic, vocab_size)\n",
    "        \n",
    "        posdis = Disent.posdis(torch.from_numpy(objects), messages)\n",
    "        bosdis = Disent.bosdis(torch.from_numpy(objects), messages, vocab_size)\n",
    "        \n",
    "        posdis_bosdis['posdis_specific'] = posdis_specific\n",
    "        posdis_bosdis['bosdis_specific'] = bosdis_specific\n",
    "        posdis_bosdis['posdis_generic'] = posdis_generic\n",
    "        posdis_bosdis['bosdis_generic'] = bosdis_generic\n",
    "        posdis_bosdis['posdis'] = posdis\n",
    "        posdis_bosdis['bosdis'] = bosdis\n",
    "\n",
    "        print(posdis_bosdis)\n",
    "    \n",
    "        if pickle_data:\n",
    "            pickle.dump(posdis_bosdis, open(path_to_run + \"posdis_bosdis.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Posdis and bosdis concept x context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T16:21:47.222765Z",
     "start_time": "2024-02-28T16:21:29.736013Z"
    }
   },
   "outputs": [],
   "source": [
    "# bosdis concept x context\n",
    "for d in range(len(datasets)):\n",
    "\n",
    "    vs_factor = int(paths[d][-2])\n",
    "    vocab_size = (n_values[d] + 1) * vs_factor + 1\n",
    "    \n",
    "    for run in range(5):\n",
    "\n",
    "        path_to_run = paths[d] + '/' + str(setting) +'/' + str(run) + '/' \n",
    "        path_to_interaction = (path_to_run + 'interactions/train/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "        interaction = torch.load(path_to_interaction)\n",
    "\n",
    "        attributes = n_attributes[d]\n",
    "        values = n_values[d]\n",
    "        scores = bosdis(interaction, attributes, values, vocab_size)\n",
    "\n",
    "        if pickle_data:\n",
    "            pickle.dump(scores, open(path_to_run + 'bosdis_scores.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T16:22:09.615176Z",
     "start_time": "2024-02-28T16:21:47.223647Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# posdis concept x context\n",
    "for d in range(len(datasets)):\n",
    "\n",
    "    vs_factor = int(paths[d][-2])\n",
    "    vocab_size = (n_values[d] + 1) * vs_factor + 1\n",
    "\n",
    "    for run in range(5):\n",
    "        path_to_run = paths[d] + '/' + str(setting) + '/' + str(run) + '/'\n",
    "        print(path_to_run)\n",
    "        path_to_interaction = (path_to_run + 'interactions/train/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "        #print(path_to_interaction)\n",
    "        interaction = torch.load(path_to_interaction)\n",
    "\n",
    "        attributes = n_attributes[d]\n",
    "        values = n_values[d]\n",
    "        scores = posdis(interaction, attributes, values, vocab_size)\n",
    "\n",
    "        if pickle_data:\n",
    "            pickle.dump(scores, open(path_to_run + 'posdis_scores.pkl', 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## co-occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not yet implemented:\n",
    "\n",
    "for d in range(len(datasets)):\n",
    "    \n",
    "    vs_factor = int(paths[d][-2])\n",
    "    \n",
    "    for run in range(5): \n",
    "        \n",
    "        path_to_run = paths[d] + str(setting) +'/' + str(run) + '/'\n",
    "        path_to_interaction = (path_to_run + 'interactions/train/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "        interaction = torch.load(path_to_interaction)\n",
    "\n",
    "        attributes = n_attributes[d]\n",
    "        values = n_values[d]\n",
    "        \n",
    "        scores = cooccurrence_per_hierarchy_level(interaction, attributes, values, vs_factor)\n",
    "\n",
    "        print(scores)\n",
    "        \n",
    "        if pickle_data:\n",
    "            pickle.dump(scores, open(path_to_run + 'normalized_cooccurrence.pkl', 'wb'))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emergab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
